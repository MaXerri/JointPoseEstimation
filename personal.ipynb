{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets.custom_datasets import LSPDataset, MPIIDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from models.pose_estimation_model import TransformerPoseModel\n",
    "from utils.training_helpers import resize_single_joint\n",
    "# from utils.training_helpers import plot_with_joints\n",
    "from utils.training_helpers import plot_with_joints_r\n",
    "from utils.preprocessing_helpers import get_image_sizes, get_list_of_image_names\n",
    "from models.loss import JointsMSELoss\n",
    "from torch.optim import Adam\n",
    "from utils.heatmap_funcs import generate_gaussian_heatmap, generate_single_image_gaussian, upsample_heatmap\n",
    "import torchvision.io\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "# annot = np.load('/home/mxerri/JointPoseEstimation/Data/lsp/leeds_sports_extended.npy')\n",
    "annot = np.load('/Users/richardpignatiello/repos/4701/JointPoseEstimation/data/lsp/leeds_sports_extended.npy')\n",
    "annot_s = np.swapaxes(annot, 0,2)\n",
    "annot_s = np.swapaxes(annot_s,1,2)\n",
    "\n",
    "# retrieve image sizes\n",
    "# image_sizes = get_image_sizes('/home/mxerri/JointPoseEstimation/Data/lsp/images/')\n",
    "image_sizes = get_image_sizes('/Users/richardpignatiello/repos/4701/JointPoseEstimation/data/lsp/images/')\n",
    "# image_sizes_resized = get_image_sizes('/home/mxerri/JointPoseEstimation/Data/lsp/images224/')\n",
    "image_sizes_resized = get_image_sizes('/Users/richardpignatiello/repos/4701/JointPoseEstimation/data/lsp/images224/')\n",
    "annot_resize = np.zeros_like(annot_s)\n",
    "\n",
    "# resize annotations\n",
    "for i in range(10000):\n",
    "    annot_resize[i] = resize_single_joint(annot_s[i],image_sizes_resized[i],image_sizes[i] ) \n",
    "    # make annortations into 56 x 56 for loss function \n",
    "    annot_resize[i] = resize_single_joint(annot_resize[i],(56,56),(224,224))\n",
    "\n",
    "print(annot_resize.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 14, 56, 56)\n",
      "(56, 56)\n",
      "(18, 40)\n",
      "[18. 40.  1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGfCAYAAAD22G0fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeFUlEQVR4nO3df0yd5f3/8ddNgQNUONpZz4EUDU7mZn9F2w0hapkKST/O1HR/qDWmi/+oraakWzqxf4iLgbYmpC5ol7rF1Swd+2NWTTYdJLN0C98mtJZI0Jkusspij0xTz0FKDwWu7x/9eD4i5z7utOCbc87zkdyJXNd9c1/nAs+r1+F9ruM555wAADCQZz0AAEDuIoQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZvLn6xu/8MILevbZZ3X69GktX75ce/fu1a233vq1101PT+ujjz5SaWmpPM+br+EBAOaJc06jo6OqqKhQXt7XrHXcPOjs7HQFBQXuxRdfdO+++67btm2bW7x4sTt16tTXXjs8POwkcXBwcHBk+DE8PPy1z/mec3O/gWlNTY1uuukm7du3L9H2ve99T/fcc4/a2tpSXhuNRnX55ZfrFv2P8lUw10MDAMyzSZ3X3/VnffbZZwoGgynPnfOX4yYmJnT8+HE98cQTM9obGxvV29s76/x4PK54PJ74enR09H8HVqB8jxACgIzzv0ub/+ZPKnNemPDJJ59oampKoVBoRnsoFFIkEpl1fltbm4LBYOKorKyc6yEBABaoeauO+2oCOueSpmJzc7Oi0WjiGB4enq8hAQAWmDl/Oe7KK6/UokWLZq16RkZGZq2OJCkQCCgQCMz1MAAAGWDOV0KFhYVas2aNuru7Z7R3d3errq5urm8HAMhg8/I+oe3bt+vBBx/U2rVrVVtbq/379+vDDz/UI488Mh+3AwBkqHkJoXvvvVeffvqpfvGLX+j06dNasWKF/vznP+uaa66Zj9sBADLUvLxP6FLEYjEFg0HVawMl2gCQgSbdeR3Wa4pGoyorK0t5LnvHAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMJNvPQAAWcLzLuKaBfDvYDd9Ede4uR9HjloAvwEAgFxFCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMykHUJHjhzR3XffrYqKCnmep1dffXVGv3NOLS0tqqioUHFxserr6zU4ODhX4wVgyfN8D2/RouRHfoH/UZDve+QVFiQ/igL+h881qe6Tcnw+jynVPCA9aYfQ2NiYVq9erY6OjqT9e/bsUXt7uzo6OtTX16dwOKyGhgaNjo5e8mABANkl7Terrl+/XuvXr0/a55zT3r17tXPnTm3cuFGSdODAAYVCIR08eFAPP/zwpY0WAJBV5vRvQkNDQ4pEImpsbEy0BQIBrVu3Tr29vUmvicfjisViMw4AQG6Y0xCKRCKSpFAoNKM9FAol+r6qra1NwWAwcVRWVs7lkAAAC9i8VMd5X/njnHNuVtsXmpubFY1GE8fw8PB8DAkAsADN6Qam4XBY0oUVUXl5eaJ9ZGRk1uroC4FAQIFAYC6HAQDIEHMaQlVVVQqHw+ru7taNN94oSZqYmFBPT4927949l7cCcKlSlRP77G7tLVrkf8kin2sKC/3vU5DiKSjP5155KcY9nXx3a296yv+a85O+XW5iInlHit2/3VSKe/nt2J3Du3KnHUKff/65/vnPfya+HhoaUn9/v5YsWaKrr75aTU1Nam1tVXV1taqrq9Xa2qqSkhJt2rRpTgcOAMh8aYfQsWPH9MMf/jDx9fbt2yVJmzdv1m9/+1vt2LFD4+Pj2rJli86cOaOamhp1dXWptLR07kYNAMgKnnMLax0Yi8UUDAZVrw3K9wqshwNkrxx6OU5z/HKcm/L/IDxejpMm3Xkd1muKRqMqKytLeS57xwEAzBBCAAAzc1odByCDpKjw8nvZzSv0f4ncKy7yaS/2vcYVpXipzudebpH/y3HelM/LWhPn/a8551MBJ0nj4z7t5/yvSfHtnN8rdb4d2Y+VEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQ4k2kM1S7IqQcvcDn/Jor6TE/17By5I2TwX9rzkfTF7WLUmTJcnH51I8a3k+mx/kn/UvgS6I+pdb50V95iFFebt01r/Lp3zb+e2kcKEzxb0yHyshAIAZQggAYIYQAgCYIYQAAGYIIQCAGarjgGzgUwV3MZ//I/lvRupXASdJk0uTf27MuVDA95qzS/3HFw8mf0zTKT5mLM9nn9JA1P8+Jf/x7ysqTP4UmeqJ00tV6eb3WUPOfwy+n0+UJVVzrIQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlKtIFslmKjTa+w0L+vuDhpe6rNSP1KsaPX+D/NjC3zLzOeXhpP2p5f5FOHLWnyXPL67fH/+D/W8yXpPw2WTPjPw6K4//g0kbzPTaUo65b/5qvZgJUQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDiTaQDfxKsfOS70QtSSrw/9/fFSUvaT4f9NldW/47Yqcqw15c/Zlv39rwcNL28qKo7zWnzwWTth+7vNL3mjFd7ttXcDb5YyqI+c9DXmzct09nfeY8nuLn5PezddlRus1KCABghhACAJghhAAAZgghAIAZQggAYIbqOCCLeV6Kqqu85JVfkqTC5BuBTpb4XxMPJr+X30akkn8FnCRtWvr/krZfm+9fHffBZPLquFTe+sx/M9L4x8k3ZU01DwGfuZPkO+epfk7+tYXZgZUQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDiTaQzfJS/DszxeamblHyPpfiGWPapzI5v+i87zWpNiP1K8WuKrjMfxBKfk2q+6Qa33RB8hLtVPPgN3eS5PnNeaqfU5bL3UcOADBHCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMxQog1ks+npFH3++zN7U8n7vEn/b5fnU+k8ec5/V+nT5/x3vfbfETv9XbRT3SfV+AI+jynVPPjNnST/OU/1c8pyrIQAAGYIIQCAGUIIAGCGEAIAmCGEAABm0qqOa2tr0yuvvKJ//OMfKi4uVl1dnXbv3q3rr78+cY5zTk8//bT279+vM2fOqKamRs8//7yWL18+54MHkJpzKSrgpqf8L5xIXhaWf9b/mkB0UdL28f8U+l5z7PJK/zH4SLUZqV8V3LGI/33yUowvEE0+f6nmwW/uJEk+c57q55Tt0loJ9fT0aOvWrTp69Ki6u7s1OTmpxsZGjY2NJc7Zs2eP2tvb1dHRob6+PoXDYTU0NGh0dHTOBw8AyGxprYTefPPNGV+/9NJLuuqqq3T8+HHddtttcs5p79692rlzpzZu3ChJOnDggEKhkA4ePKiHH3547kYOAMh4l/Q3oWj0wrJ4yZIlkqShoSFFIhE1NjYmzgkEAlq3bp16e3uTfo94PK5YLDbjAADkhosOIeectm/frltuuUUrVqyQJEUiEUlSKBSacW4oFEr0fVVbW5uCwWDiqKxM/zViAEBmuugQeuyxx/TOO+/o97///aw+z5v56YHOuVltX2hublY0Gk0cw8PDFzskAECGuai94x5//HG9/vrrOnLkiJYtW5ZoD4fDki6siMrLyxPtIyMjs1ZHXwgEAgoEkn+ELgAgu6UVQs45Pf744zp06JAOHz6sqqqqGf1VVVUKh8Pq7u7WjTfeKEmamJhQT0+Pdu/ePXejBjCT89kAM8UmpTrvvwund24iaXtB9JzvNSX/SV6ifb7E/2lmTJf79r31WUnS9vwi/xJov81IU5VhL/538ldpJKnkP8nnKNU8+M2dJDm/OU/1c/L72WaJtEJo69atOnjwoF577TWVlpYm/s4TDAZVXFwsz/PU1NSk1tZWVVdXq7q6Wq2trSopKdGmTZvm5QEAADJXWiG0b98+SVJ9ff2M9pdeekk/+clPJEk7duzQ+Pi4tmzZknizaldXl0pLS+dkwACA7JH2y3Ffx/M8tbS0qKWl5WLHBADIEewdBwAwQwgBAMzw8d5ANktRWeUm/Ku4ND6etDkv6v9R2EWF6T+dFJxNXlEnSfGPk791Y7rA/y0dfh/H7bcRqeRfASdJRR/Hk7bnRc/6XuN85k5KMedZXgGXCishAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEm0gG/jsZuKmpvyv8VL8G3Q8+QadXopr/J5MSiaSb0QqSQWxIt++yZLk5dsuxbOW51NtnX/Wfx5SbUbqW4od/dz3Guczd5LkppKXYqf8Of0XO9VkMlZCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMJdpANktR3puyLNh3g23/3aM9n52gF8V9traWlBfz33E6UJh8x263yPMfw5TP453wH4N3zn83cb8dsVOWYae4l++cZ3kZdiqshAAAZgghAIAZQggAYIYQAgCYIYQAAGaojgNylU81myQ5v8I5/0Iyya/yK0W1mM6meArKS76BqZfnXx2naZ8qs2n/SkB33mfXU0luIvkD9tuI9EJfqs1I/a/LVayEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZSrSBXJVq00yfGm2XqsTYJS+pTlXOrHiKzUg9n768FP92nk5+L5fqsfqVdUu+JdWpy7BzdzPSi8FKCABghhACAJghhAAAZgghAIAZQggAYIbqOAD/vYv6uPAUlWSe/7+Dv7Eas4vZVJQKuDnDSggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmKFEG8DcuJiyZZ+NUpE7WAkBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwk1YI7du3T6tWrVJZWZnKyspUW1urN954I9HvnFNLS4sqKipUXFys+vp6DQ4OzvmgAQDZIa0QWrZsmXbt2qVjx47p2LFjuv3227Vhw4ZE0OzZs0ft7e3q6OhQX1+fwuGwGhoaNDo6Oi+DBwBkNs+5i9l//f8sWbJEzz77rB566CFVVFSoqalJP//5zyVJ8XhcoVBIu3fv1sMPP/xffb9YLKZgMKh6bVC+V3ApQwMAGJh053VYrykajaqsrCzluRf9N6GpqSl1dnZqbGxMtbW1GhoaUiQSUWNjY+KcQCCgdevWqbe31/f7xONxxWKxGQcAIDekHUIDAwO67LLLFAgE9Mgjj+jQoUO64YYbFIlEJEmhUGjG+aFQKNGXTFtbm4LBYOKorKxMd0gAgAyVdghdf/316u/v19GjR/Xoo49q8+bNevfddxP9nufNON85N6vty5qbmxWNRhPH8PBwukMCAGSotD/eu7CwUNddd50kae3aterr69Nzzz2X+DtQJBJReXl54vyRkZFZq6MvCwQCCgQC6Q4DAJAFLvl9Qs45xeNxVVVVKRwOq7u7O9E3MTGhnp4e1dXVXeptAABZKK2V0JNPPqn169ersrJSo6Oj6uzs1OHDh/Xmm2/K8zw1NTWptbVV1dXVqq6uVmtrq0pKSrRp06b5Gj8AIIOlFUIff/yxHnzwQZ0+fVrBYFCrVq3Sm2++qYaGBknSjh07ND4+ri1btujMmTOqqalRV1eXSktL52XwAIDMdsnvE5prvE8IADLbN/I+IQAALhUhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMXFIItbW1yfM8NTU1Jdqcc2ppaVFFRYWKi4tVX1+vwcHBSx0nACALXXQI9fX1af/+/Vq1atWM9j179qi9vV0dHR3q6+tTOBxWQ0ODRkdHL3mwAIDsclEh9Pnnn+uBBx7Qiy++qCuuuCLR7pzT3r17tXPnTm3cuFErVqzQgQMHdPbsWR08eHDOBg0AyA4XFUJbt27VXXfdpTvvvHNG+9DQkCKRiBobGxNtgUBA69atU29vb9LvFY/HFYvFZhwAgNyQn+4FnZ2devvtt9XX1zerLxKJSJJCodCM9lAopFOnTiX9fm1tbXr66afTHQYAIAuktRIaHh7Wtm3b9Lvf/U5FRUW+53meN+Nr59ysti80NzcrGo0mjuHh4XSGBADIYGmthI4fP66RkRGtWbMm0TY1NaUjR46oo6ND77//vqQLK6Ly8vLEOSMjI7NWR18IBAIKBAIXM3YAQIZLayV0xx13aGBgQP39/Ylj7dq1euCBB9Tf369rr71W4XBY3d3diWsmJibU09Ojurq6OR88ACCzpbUSKi0t1YoVK2a0LV68WN/61rcS7U1NTWptbVV1dbWqq6vV2tqqkpISbdq0ae5GDQDICmkXJnydHTt2aHx8XFu2bNGZM2dUU1Ojrq4ulZaWzvWtAAAZznPOOetBfFksFlMwGFS9NijfK7AeDgAgTZPuvA7rNUWjUZWVlaU8l73jAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYCatEGppaZHneTOOcDic6HfOqaWlRRUVFSouLlZ9fb0GBwfnfNAAgOyQ9kpo+fLlOn36dOIYGBhI9O3Zs0ft7e3q6OhQX1+fwuGwGhoaNDo6OqeDBgBkh7RDKD8/X+FwOHEsXbpU0oVV0N69e7Vz505t3LhRK1as0IEDB3T27FkdPHhwzgcOAMh8aYfQyZMnVVFRoaqqKt1333364IMPJElDQ0OKRCJqbGxMnBsIBLRu3Tr19vb6fr94PK5YLDbjAADkhrRCqKamRi+//LL+8pe/6MUXX1QkElFdXZ0+/fRTRSIRSVIoFJpxTSgUSvQl09bWpmAwmDgqKysv4mEAADJRWiG0fv16/fjHP9bKlSt155136k9/+pMk6cCBA4lzPM+bcY1zblbblzU3NysajSaO4eHhdIYEAMhgl1SivXjxYq1cuVInT55MVMl9ddUzMjIya3X0ZYFAQGVlZTMOAEBuuKQQisfjeu+991ReXq6qqiqFw2F1d3cn+icmJtTT06O6urpLHigAIPvkp3Pyz372M9199926+uqrNTIyomeeeUaxWEybN2+W53lqampSa2urqqurVV1drdbWVpWUlGjTpk3zNX4AQAZLK4T+/e9/6/7779cnn3yipUuX6uabb9bRo0d1zTXXSJJ27Nih8fFxbdmyRWfOnFFNTY26urpUWlo6L4MHAGQ2zznnrAfxZbFYTMFgUPXaoHyvwHo4AIA0TbrzOqzXFI1Gv/bv/OwdBwAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMBMvvUAvso5J0ma1HnJGQ8GAJC2SZ2X9H/P56ksuBAaHR2VJP1dfzYeCQDgUoyOjioYDKY8x3P/TVR9g6anp/XRRx+ptLRUnucpFoupsrJSw8PDKisrsx6eGebhAubhAubhAubhgoU2D845jY6OqqKiQnl5qf/qs+BWQnl5eVq2bNms9rKysgUxudaYhwuYhwuYhwuYhwsW0jx83QroCxQmAADMEEIAADMLPoQCgYCeeuopBQIB66GYYh4uYB4uYB4uYB4uyOR5WHCFCQCA3LHgV0IAgOxFCAEAzBBCAAAzhBAAwAwhBAAws6BD6IUXXlBVVZWKioq0Zs0a/e1vf7Me0rw6cuSI7r77blVUVMjzPL366qsz+p1zamlpUUVFhYqLi1VfX6/BwUGbwc6jtrY2ff/731dpaamuuuoq3XPPPXr//fdnnJMLc7Fv3z6tWrUq8S742tpavfHGG4n+XJiDZNra2uR5npqamhJtuTAXLS0t8jxvxhEOhxP9mToHCzaE/vCHP6ipqUk7d+7UiRMndOutt2r9+vX68MMPrYc2b8bGxrR69Wp1dHQk7d+zZ4/a29vV0dGhvr4+hcNhNTQ0JDZ9zRY9PT3aunWrjh49qu7ubk1OTqqxsVFjY2OJc3JhLpYtW6Zdu3bp2LFjOnbsmG6//XZt2LAh8cSSC3PwVX19fdq/f79WrVo1oz1X5mL58uU6ffp04hgYGEj0ZewcuAXqBz/4gXvkkUdmtH33u991TzzxhNGIvlmS3KFDhxJfT09Pu3A47Hbt2pVoO3funAsGg+5Xv/qVwQi/OSMjI06S6+npcc7l9lxcccUV7te//nVOzsHo6Kirrq523d3dbt26dW7btm3Oudz5fXjqqafc6tWrk/Zl8hwsyJXQxMSEjh8/rsbGxhntjY2N6u3tNRqVraGhIUUikRlzEggEtG7duqyfk2g0KklasmSJpNyci6mpKXV2dmpsbEy1tbU5OQdbt27VXXfdpTvvvHNGey7NxcmTJ1VRUaGqqirdd999+uCDDyRl9hwsuF20JemTTz7R1NSUQqHQjPZQKKRIJGI0KltfPO5kc3Lq1CmLIX0jnHPavn27brnlFq1YsUJSbs3FwMCAamtrde7cOV122WU6dOiQbrjhhsQTSy7MgSR1dnbq7bffVl9f36y+XPl9qKmp0csvv6zvfOc7+vjjj/XMM8+orq5Og4ODGT0HCzKEvuB53oyvnXOz2nJNrs3JY489pnfeeUd///vfZ/Xlwlxcf/316u/v12effaY//vGP2rx5s3p6ehL9uTAHw8PD2rZtm7q6ulRUVOR7XrbPxfr16xP/vXLlStXW1urb3/62Dhw4oJtvvllSZs7Bgnw57sorr9SiRYtmrXpGRkZmJX2u+KIKJpfm5PHHH9frr7+ut956a8ZnTOXSXBQWFuq6667T2rVr1dbWptWrV+u5557LqTk4fvy4RkZGtGbNGuXn5ys/P189PT365S9/qfz8/MTjzYW5+LLFixdr5cqVOnnyZEb/PizIECosLNSaNWvU3d09o727u1t1dXVGo7JVVVWlcDg8Y04mJibU09OTdXPinNNjjz2mV155RX/9619VVVU1oz+X5uKrnHOKx+M5NQd33HGHBgYG1N/fnzjWrl2rBx54QP39/br22mtzZi6+LB6P67333lN5eXlm/z6YlUR8jc7OTldQUOB+85vfuHfffdc1NTW5xYsXu3/961/WQ5s3o6Oj7sSJE+7EiRNOkmtvb3cnTpxwp06dcs45t2vXLhcMBt0rr7ziBgYG3P333+/Ky8tdLBYzHvncevTRR10wGHSHDx92p0+fThxnz55NnJMLc9Hc3OyOHDnihoaG3DvvvOOefPJJl5eX57q6upxzuTEHfr5cHedcbszFT3/6U3f48GH3wQcfuKNHj7of/ehHrrS0NPGcmKlzsGBDyDnnnn/+eXfNNde4wsJCd9NNNyVKdLPVW2+95STNOjZv3uycu1CG+dRTT7lwOOwCgYC77bbb3MDAgO2g50GyOZDkXnrppcQ5uTAXDz30UOL3f+nSpe6OO+5IBJBzuTEHfr4aQrkwF/fee68rLy93BQUFrqKiwm3cuNENDg4m+jN1Dvg8IQCAmQX5NyEAQG4ghAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgJn/D6UuyACEi3vEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert to joint heatmaps from coordiantes: \n",
    "heatmaps = generate_gaussian_heatmap(annot_resize[0:10],(56,56),3)\n",
    "print(heatmaps.shape)\n",
    "# heatmaps_down = upsample_heatmap(heatmaps, (56,56))\n",
    "plt.imshow(heatmaps[0][2])\n",
    "print(heatmaps[0][2].shape)\n",
    "z = np.unravel_index(heatmaps[0,2,:,:].argmax(), heatmaps[0,2,:,:].shape)\n",
    "print(z)\n",
    "print(annot_resize[0,2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_plot():\n",
    "  # im = torchvision.io.read_image('/Users/richardpignatiello/repos/4701/JointPoseEstimation/data/lsp/images224/resized_im00012.jpg')\n",
    "  im = torchvision.io.read_image('/Users/richardpignatiello/Downloads/jpe_test/thomas.jpg')\n",
    "  im = torch.unsqueeze(im, 0)\n",
    "  im = im.to(mps_device)\n",
    "\n",
    "  joints = model(im, False)\n",
    "  # displayHeatmap(joints)\n",
    "  joints = joints.squeeze()\n",
    "  im = im.squeeze()\n",
    "  x = []\n",
    "  y = []\n",
    "  for joint in joints:\n",
    "    # print(f\"max: {(joint==torch.max(joint)).nonzero()}\")\n",
    "    coor = (joint==torch.max(joint)).nonzero()\n",
    "    print(torch.max(joint))\n",
    "    if torch.max(joint) > .3:\n",
    "      x.append(int(coor[0][0] * 4))\n",
    "      y.append(int(coor[0][1] * 4))\n",
    "      print((int(coor[0][0] * 4), int(coor[0][1] * 4)))\n",
    "  im = im.cpu()\n",
    "  plot_with_joints_r(im, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayHeatmap(heatmap):\n",
    "  print('show image!!')\n",
    "  for i in range(14):\n",
    "    plt.imshow(heatmap[0][i].detach().cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(save_path):\n",
    "    num_batches = len(train_loader)\n",
    "    for epoch in range(200):\n",
    "        start_epoch = datetime.datetime.now()\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        load_tensor_total = datetime.timedelta()\n",
    "        model_computation_total = datetime.timedelta()\n",
    "        calc_loss_total = datetime.timedelta()\n",
    "        loss_backward_total = datetime.timedelta()\n",
    "        optimizer_time_total = datetime.timedelta()\n",
    "        for batch_idx, (imgs, labels, _) in enumerate(train_loader):\n",
    "            load_tensors_start = datetime.datetime.now()\n",
    "            imgs = imgs.to(mps_device)\n",
    "            labels = labels.float()\n",
    "            labels = labels.to(mps_device)\n",
    "            load_tensor_total += datetime.datetime.now() - load_tensors_start\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            computation_start = datetime.datetime.now()\n",
    "            output = model(imgs, 'train') # -> (5, H/4, W/4, #joints) \n",
    "            computation_time_total = datetime.datetime.now() - computation_start\n",
    "\n",
    "            calc_loss_start = datetime.datetime.now()\n",
    "            loss = loss_func(output, labels.float())\n",
    "            calc_loss_total += datetime.datetime.now() - calc_loss_start\n",
    "            \n",
    "            loss_backward_start = datetime.datetime.now()\n",
    "            loss.backward()\n",
    "            loss_backward_total += datetime.datetime.now() - loss_backward_start\n",
    "\n",
    "            optimizer_start_time = datetime.datetime.now()\n",
    "            optimizer.step()\n",
    "            optimizer_time_total = datetime.datetime.now() - optimizer_start_time\n",
    "            total_loss += loss\n",
    "\n",
    "            # print(\"batch completed\")\n",
    "            # if batch_idx % 31 == 0 and batch_idx > 0:\n",
    "                # print(f\"{int(batch_idx / len(train_loader) * 100) + 1}%\")\n",
    "                # print(f\"{int(batch_idx / 625 * 100) + 1}%\")\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"epoch:\", epoch, \"loss: \", total_loss)\n",
    "            elapsed_epoch = datetime.datetime.now() - start_epoch\n",
    "            print(f\"epoch {epoch} trained in: {elapsed_epoch}\")\n",
    "            print(f\"AVG time per batch {elapsed_epoch / num_batches}\")\n",
    "            print(f\"AVG tensor load time: {load_tensor_total / num_batches}\")\n",
    "            print(f\"AVG prediction time: {computation_time_total / num_batches}\")\n",
    "            print(f\"AVG loss calc time: {calc_loss_total / num_batches}\")\n",
    "            print(f\"AVG loss backward time: {loss_backward_total / num_batches}\")\n",
    "            print(f\"AVG optimizer step time: {optimizer_time_total / num_batches}\")\n",
    "            break\n",
    "            # displayHeatmap(output)\n",
    "            # test_plot()\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "            # torch.save(model.state_dict(), save_path)\n",
    "            # displayHeatmap(output)\n",
    "            # test_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "  start_time = datetime.timedelta()\n",
    "  total_loss = 0\n",
    "  for batch_idx, (imgs, labels, _) in enumerate(test_loader):\n",
    "    imgs = imgs.to(mps_device)\n",
    "    labels = labels.float()\n",
    "    labels = labels.to(mps_device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(imgs, 'val') # -> (5, H/4, W/4, #joints) \n",
    "\n",
    "    loss = loss_func(output, labels.float())\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    total_loss += loss\n",
    "\n",
    "  elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "  print(f\"finished validation in {elapsed_time}\")\n",
    "  print(f\"loss: {total_loss}\")\n",
    "  print(f\"loss x 4: {total_loss * 4}\") # because validation data is 1/4 train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create officia datasets and dataloaders for training\n",
    "image_name_list = get_list_of_image_names('/Users/richardpignatiello/repos/4701/JointPoseEstimation/data/lsp/images224/')\n",
    "dataset = LSPDataset(annot_resize, image_name_list,\"/Users/richardpignatiello/repos/4701/JointPoseEstimation/data/lsp/images224/\",1)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [8000, 2000])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False) # changed 32 -> 16\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # changed 32 -> 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/jpe/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss:  tensor(1.2300, device='mps:0', grad_fn=<AddBackward0>)\n",
      "epoch 0 trained in: 0:01:56.052478\n",
      "AVG time per batch 0:00:00.464210\n",
      "AVG tensor load time: 0:00:00.369159\n",
      "AVG prediction time: 0:00:00.000050\n",
      "AVG loss calc time: 0:00:00.006625\n",
      "AVG loss backward time: 0:00:00.014356\n",
      "AVG optimizer step time: 0:00:00.000095\n",
      "finished validation in 2023-12-07 19:46:25.771386\n",
      "loss: 0.12339607626199722\n",
      "loss x 4: 0.4935843050479889\n"
     ]
    }
   ],
   "source": [
    "model = TransformerPoseModel(2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = JointsMSELoss()\n",
    "model.to(mps_device)\n",
    "\n",
    "train('asdf')\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create officia datasets and dataloaders for training\n",
    "# dataset = LSPDataset(annot_resize,\"/Users/richardpignatiello/repos/4701/JointPoseEstimation/data/lsp/images224/\", 3)\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [8000, 2000])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False) # changed 32 -> 16\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False) # changed 32 -> 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerPoseModel(2)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# loss_func = JointsMSELoss()\n",
    "# model.to(mps_device)\n",
    "\n",
    "# train('trained_models/small_blur.pth')\n",
    "# validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerPoseModel(2)\n",
    "# model.load_state_dict(torch.load('/Users/richardpignatiello/repos/4701/JointPoseEstimation/trained_models/small_blur.pth'))\n",
    "# mps_device = torch.device(\"mps\")\n",
    "# model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_loader = DataLoader(test_dataset, 1, False)\n",
    "# for batch_idx, (imgs, labels, path) in enumerate(vis_loader):\n",
    "#   path = path[0]\n",
    "#   if batch_idx == 50:\n",
    "#     break\n",
    "#   im = torchvision.io.read_image(path)\n",
    "#   im = torch.unsqueeze(im, 0)\n",
    "#   im = im.to(mps_device)\n",
    "\n",
    "#   joints = model(im, 'pred')\n",
    "#   joints = joints.squeeze()\n",
    "#   im = im.squeeze()\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for joint in joints:\n",
    "#     # print(f\"max: {(joint==torch.max(joint)).nonzero()}\")\n",
    "#     coor = (joint==torch.max(joint)).nonzero()\n",
    "#     x.append(int(coor[0][0] * 4))\n",
    "#     y.append(int(coor[0][1] * 4))\n",
    "#   im = im.cpu()\n",
    "#   plot_with_joints_r(im, x, y)\n",
    "  \n",
    "#   # print(x)\n",
    "#   # print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#   im = torchvision.io.read_image(f'/Users/richardpignatiello/Downloads/jpe_test/IMG_847{i}.jpg')\n",
    "#   im = torch.unsqueeze(im, 0)\n",
    "#   im = im.to(mps_device)\n",
    "\n",
    "#   joints = model(im, 'pred)\n",
    "#   joints = joints.squeeze()\n",
    "#   im = im.squeeze()\n",
    "#   x = []\n",
    "#   y = []\n",
    "#   for joint in joints:\n",
    "#     # print(f\"max: {(joint==torch.max(joint)).nonzero()}\")\n",
    "#     coor = (joint==torch.max(joint)).nonzero()\n",
    "#     x.append(int(coor[0][0] * 4))\n",
    "#     y.append(int(coor[0][1] * 4))\n",
    "#   im = im.cpu()\n",
    "#   plot_with_joints_r(im, x, y)\n",
    "#   # print(x)\n",
    "#   # print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
