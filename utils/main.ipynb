{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r7VNsIuXALn2"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'embeddings'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/richardpignatiello/repos/4701/JointPoseEstimation/utils/main.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardpignatiello/repos/4701/JointPoseEstimation/utils/main.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m read_image\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardpignatiello/repos/4701/JointPoseEstimation/utils/main.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/richardpignatiello/repos/4701/JointPoseEstimation/utils/main.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m PatchEmbeddings\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'embeddings'"
          ]
        }
      ],
      "source": [
        "# packages\n",
        "import numpy as np\n",
        "import torch\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from matplotlib import pyplot as plt\n",
        "import sys\n",
        "sys.path.append('/Users/richardpignatiello/repos/4701/JointPoseEstimation/models/')\n",
        "from embeddings import PatchEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wwNHvf1Un6z",
        "outputId": "413dab04-c151-4eb8-87c3-e53d4b3976ec"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIjbi8vZGlre"
      },
      "outputs": [],
      "source": [
        "# Non Augmented Custom Dataset\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class LSPDataset():\n",
        "  # image_labels: List[Dict] of the filtered mpii annolist\n",
        "  def __init__(self, image_labels, img_dir):\n",
        "    self.image_labels = image_labels\n",
        "    self.img_dir = img_dir\n",
        "    # can add transforms if we need/want\n",
        "\n",
        "  def __len__(self):\n",
        "    return (self.image_labels.shape[0])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # inputs\n",
        "    # idx: int -> index of image sample\n",
        "    #\n",
        "    # image: torch.Tensor -> pytorch tensor representing image specified\n",
        "    # label: list[dict] -> each dict contains x and y coordinate, joint id, and whether the joint is visible\n",
        "\n",
        "    # img_path = self.img_dir + self.image_labels[idx]['image']['name']\n",
        "    img_path = self.img_dir + \"resized_im\" + '0'*(5-len(str(idx+1))) + str(idx + 1) + \".jpg\"\n",
        "    image = read_image(img_path)\n",
        "\n",
        "    label = self.image_labels[idx]\n",
        "\n",
        "    return image, label\n",
        "\n",
        "class MPIIDataset():\n",
        "  # image_labels: List[Dict] of the filtered mpii annolist\n",
        "  def __init__(self, image_labels, img_dir):\n",
        "    self.image_labels = image_labels\n",
        "    self.img_dir = img_dir\n",
        "    # can add transforms if we need/want\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # inputs\n",
        "    # idx: int -> index of image sample\n",
        "    #\n",
        "    # image: torch.Tensor -> pytorch tensor representing image specified\n",
        "    # label: list[dict] -> each dict contains x and y coordinate, joint id, and whether the joint is visible\n",
        "\n",
        "    # img_path = self.img_dir + self.image_labels[idx]['image']['name']\n",
        "    img_path = self.img_dir + \"r_im\" + '0'*(5-len(str(idx))) + \".jpg\"\n",
        "    image = read_image(img_path)\n",
        "\n",
        "    label = self.image_labels['annorect']['annopoints']['point']\n",
        "\n",
        "    return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG2DnaN0O6GM"
      },
      "outputs": [],
      "source": [
        "def get_image_sizes(folder_path):\n",
        "    \"\"\"\n",
        "    retreive the sizes of the original images\n",
        "    \"\"\"\n",
        "    sizes = []\n",
        "    image_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    # Iterate over files in the folder\n",
        "    for filename in image_files:\n",
        "\n",
        "      file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "      # Open the image using PIL\n",
        "      with Image.open(file_path) as img:\n",
        "          # Get the size of the image (width, height)\n",
        "          img_size = img.size\n",
        "          sizes.append(img_size)\n",
        "\n",
        "    return sizes\n",
        "\n",
        "def resize_joints(joints, resize_dim, original_dim):\n",
        "  \"\"\"\n",
        "  returns the new joint coordinate in the reshaped image\n",
        "  \"\"\"\n",
        "  new = np.zeros((14,3))\n",
        "  for idx, point in enumerate(joints):\n",
        "    x1 = int((point[0] * (resize_dim[0] / original_dim[0])))\n",
        "    y1 = int((point[1] * (resize_dim[1] / original_dim[1])))\n",
        "    new[idx,:] = [x1,y1,point[2]] # append resized coordinate\n",
        "  return new\n",
        "\n",
        "\n",
        "# plotting functions:\n",
        "def plot_with_joints(img, joints):\n",
        "  \"\"\"\n",
        "  Plots an image overlayed with the joints\n",
        "  \"\"\"\n",
        "\n",
        "  img = np.swapaxes(img,0,2)\n",
        "  img = np.swapaxes(img,0,1)\n",
        "  plt.scatter(joints[:,0],joints[:,1])\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5c26lVjUJpE",
        "outputId": "039671ed-fb8a-432f-ac2d-f6ab152c4d81"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXNaMLBGP6sd"
      },
      "outputs": [],
      "source": [
        "class ViT():\n",
        "  def _init_(self):\n",
        "    self.img_size = (224,224)\n",
        "    self.patch_size = (16,16)\n",
        "\n",
        "    self.embeddings = PatchEmbeddings()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.embeddings(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSaEti7QDKTn"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "#Not sure if this will handle batches but it should we may just need to make adjustments\n",
        "\n",
        "class attentionHead(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.feats = 768 #TODO abstract this\n",
        "    self.firstlin = nn.Linear(self.feats,self.feats*3)\n",
        "\n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "    x :\n",
        "    \"\"\"\n",
        "\n",
        "    dim = 768\n",
        "    x = torch.transpose(x,0,1)\n",
        "    x = self.firstlin(x)\n",
        "    x = torch.reshape(x,(12,3,196,64)) #TODO change this to 197 when CLS gets added and abstract all these numbers\n",
        "                                       #DO NOT ABSTRACT the numbers for the transposes those are encoding which dimension to move\n",
        "\n",
        "    Q,K,V = torch.unbind(x,1)\n",
        "    QK = torch.matmul(Q,torch.transpose(K,2,1))\n",
        "    QK = F.softmax(QK,dim = -1) #May need to swap dimension of softmax\n",
        "    out = torch.matmul(QK,V)\n",
        "    out = torch.transpose(out,0,1)\n",
        "    out = torch.flatten(out, start_dim = 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SIoFRD0RCpu",
        "outputId": "2fc4c8b4-6ee1-4b7b-b29d-d2fa4e7ba500"
      },
      "outputs": [],
      "source": [
        "# i = read_image('/content/drive/MyDrive/JointPoseEstimation/Data/lsp/images/im00001.jpg')\n",
        "i = read_image('/Users/richardpignatiello/repos/4701/JointPoseEstimation/data/lsp/images/im00001.jpg')\n",
        "\n",
        "resizer = transforms.Resize((224,224))\n",
        "i = resizer(i)\n",
        "patchembedder = PatchEmbeddings()\n",
        "tensor = patchembedder(i)\n",
        "\n",
        "\n",
        "attention_head = attentionHead()\n",
        "tensor = attention_head(tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cmhY_fvRCUy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4pDVz0AhhSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS3vsfM7inxs"
      },
      "outputs": [],
      "source": [
        "# get og image sizes :\n",
        "folder_path = '/content/drive/MyDrive/JointPoseEstimation/Data/lsp/images'\n",
        "sizes = get_image_sizes(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "Fzd9FcDrLyWI",
        "outputId": "0d41bcff-257d-4214-8df0-49d8087ddfb2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# testing that joints line up with images\n",
        "im = read_image('/content/drive/MyDrive/JointPoseEstimation/Data/lsp/images224/resized_im00001.jpg')\n",
        "im_og = Image.open('/content/drive/MyDrive/JointPoseEstimation/Data/lsp/images/im00001.jpg')\n",
        "annot = np.load('/content/drive/MyDrive/JointPoseEstimation/Data/lsp/leeds_sports_extended.npy')\n",
        "annot_s = np.swapaxes(annot, 0,2)\n",
        "annot_s = np.swapaxes(annot_s,1,2)\n",
        "\n",
        "\n",
        "resized_joints = resize_joints(annot_s[0],im.shape[1:],im_og.size)\n",
        "plot_with_joints(im,resized_joints)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLWOyYNKLyJX",
        "outputId": "4b768790-9b47-4ece-fcd1-9cd7ec60483e"
      },
      "outputs": [],
      "source": [
        "# dataloader stuff\n",
        "\n",
        "annot_s = np.swapaxes(annot, 0,2)\n",
        "annot_s = np.swapaxes(annot_s,1,2)\n",
        "\n",
        "dataset = LSPDataset(annot_s,\"/content/drive/MyDrive/JointPoseEstimation/Data/lsp/images224/\")\n",
        "print(annot_s.shape)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GUD5dg3PRSH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "vI58ymhaREqL",
        "outputId": "5a277bc0-1858-445a-8569-62a09d02c159"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "model = ViT()\n",
        "print(len(train_loader.dataset))\n",
        "for epoch in range(1):\n",
        "\n",
        "  acc_loss = 0\n",
        "  for img, label in (train_loader):\n",
        "    # Move the batch to the device\n",
        "    img, label = img.to(device) , label.to(device)\n",
        "    output = model()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
